{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":11838805,"datasetId":7438106,"databundleVersionId":12334504,"isSourceIdPinned":false}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"bcf314626bfa1d6e","cell_type":"markdown","source":"# Ejercicio 7: Bases de Datos Vectoriales\n\n## Objetivo de la práctica\n\nEntender el concepto de Bases de Datos Vectoriales y saber utilizar las herramientas actuales","metadata":{}},{"id":"6969f64a18e27b98","cell_type":"markdown","source":"## Parte 0: Carga del Corpus\n\nVamos a utilizar la API de Kaggle para acceder al dataset _Wikipedia Text Corpus for NLP and LLM Projects_\n\nEl corpus está disponible desde este [link](https://www.kaggle.com/datasets/gzdekzlkaya/wikipedia-text-corpus-for-nlp-and-llm-projects?utm_source=chatgpt.com)\n\n### Actividad\n\n1. Carga el corpus\n","metadata":{}},{"id":"759acb68ad40d112","cell_type":"code","source":"import kagglehub\nfrom kagglehub import KaggleDatasetAdapter","metadata":{"ExecuteTime":{"end_time":"2026-01-05T05:03:29.030339Z","start_time":"2026-01-05T05:03:28.451237Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:02:36.498667Z","iopub.execute_input":"2026-01-05T15:02:36.498932Z","iopub.status.idle":"2026-01-05T15:02:36.786905Z","shell.execute_reply.started":"2026-01-05T15:02:36.498910Z","shell.execute_reply":"2026-01-05T15:02:36.786091Z"}},"outputs":[],"execution_count":1},{"id":"ea5141c83c549f5e","cell_type":"code","source":"# Set the path to the file you'd like to load\nfile_path = \"wikipedia_text_corpus.csv\"\n\n# Load the latest version\ndf = kagglehub.dataset_load(\n  KaggleDatasetAdapter.PANDAS,\n  \"gzdekzlkaya/wikipedia-text-corpus-for-nlp-and-llm-projects\",\n  file_path,\n)\n\ndf.head()","metadata":{"ExecuteTime":{"end_time":"2026-01-05T05:03:35.012127Z","start_time":"2026-01-05T05:03:31.388852Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:02:39.524218Z","iopub.execute_input":"2026-01-05T15:02:39.524530Z","iopub.status.idle":"2026-01-05T15:02:40.964692Z","shell.execute_reply.started":"2026-01-05T15:02:39.524502Z","shell.execute_reply":"2026-01-05T15:02:40.963854Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                               text\n0           1  Anovo\\n\\nAnovo (formerly A Novo) is a computer...\n1           2  Battery indicator\\n\\nA battery indicator (also...\n2           3  Bob Pease\\n\\nRobert Allen Pease (August 22, 19...\n3           4  CAVNET\\n\\nCAVNET was a secure military forum w...\n4           5  CLidar\\n\\nThe CLidar is a scientific instrumen...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Anovo\\n\\nAnovo (formerly A Novo) is a computer...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Battery indicator\\n\\nA battery indicator (also...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Bob Pease\\n\\nRobert Allen Pease (August 22, 19...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>CAVNET\\n\\nCAVNET was a secure military forum w...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>CLidar\\n\\nThe CLidar is a scientific instrumen...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"id":"a000da740833ca58","cell_type":"markdown","source":"## Parte 1: Generación de Embeddings\n\nVamos a utilizar E5 como modelo de embeddings.\n\nLa documentación de E5 está disponible desde este [link](https://huggingface.co/intfloat/e5-base-v2)\n\n### Actividad\n\n1. Normalizar el corpus\n2. Definir una función `chunk_text`, y dividir los textos en _chunks_.\n3. Generar embeddings por cada _chunk_","metadata":{}},{"id":"639f5ff5b295666d","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport re\n\ndf = df.dropna(subset=[\"text\"]).reset_index(drop=True)\n\n# Limpieza básica\ndef normalize_text(s: str) -> str:\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    return s\n\ndf[\"text_norm\"] = df[\"text\"].astype(str).map(normalize_text)\n\ndf.head()","metadata":{"ExecuteTime":{"end_time":"2026-01-05T05:20:19.537784Z","start_time":"2026-01-05T05:20:17.790042Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:02:44.876911Z","iopub.execute_input":"2026-01-05T15:02:44.877584Z","iopub.status.idle":"2026-01-05T15:02:47.711594Z","shell.execute_reply.started":"2026-01-05T15:02:44.877552Z","shell.execute_reply":"2026-01-05T15:02:47.710834Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                               text  \\\n0           1  Anovo\\n\\nAnovo (formerly A Novo) is a computer...   \n1           2  Battery indicator\\n\\nA battery indicator (also...   \n2           3  Bob Pease\\n\\nRobert Allen Pease (August 22, 19...   \n3           4  CAVNET\\n\\nCAVNET was a secure military forum w...   \n4           5  CLidar\\n\\nThe CLidar is a scientific instrumen...   \n\n                                           text_norm  \n0  Anovo Anovo (formerly A Novo) is a computer se...  \n1  Battery indicator A battery indicator (also kn...  \n2  Bob Pease Robert Allen Pease (August 22, 1940Â...  \n3  CAVNET CAVNET was a secure military forum whic...  \n4  CLidar The CLidar is a scientific instrument u...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>text</th>\n      <th>text_norm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Anovo\\n\\nAnovo (formerly A Novo) is a computer...</td>\n      <td>Anovo Anovo (formerly A Novo) is a computer se...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Battery indicator\\n\\nA battery indicator (also...</td>\n      <td>Battery indicator A battery indicator (also kn...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Bob Pease\\n\\nRobert Allen Pease (August 22, 19...</td>\n      <td>Bob Pease Robert Allen Pease (August 22, 1940Â...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>CAVNET\\n\\nCAVNET was a secure military forum w...</td>\n      <td>CAVNET CAVNET was a secure military forum whic...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>CLidar\\n\\nThe CLidar is a scientific instrumen...</td>\n      <td>CLidar The CLidar is a scientific instrument u...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"id":"9c97a94faca1c4a1","cell_type":"code","source":"def chunk_text(text: str, max_chars: int = 800, overlap: int = 100):\n    \"\"\"\n    Chunking por caracteres.\n    max_chars ~ 600-1000 suele funcionar bien.\n    overlap ayuda a no cortar ideas a la mitad.\n    \"\"\"\n    chunks = []\n    start = 0\n    n = len(text)\n    while start < n:\n        end = min(start + max_chars, n)\n        chunk = text[start:end]\n        chunk = chunk.strip()\n        if len(chunk) > 0:\n            chunks.append(chunk)\n        if end == n:\n            break\n        start = max(0, end - overlap)\n    return chunks\n\nrecords = []\nfor i, row in df.iterrows():\n    chunks = chunk_text(row[\"text_norm\"], max_chars=800, overlap=100)\n    for j, ch in enumerate(chunks):\n        records.append({\n            \"doc_id\": int(i),\n            \"chunk_id\": j,\n            \"text\": ch\n        })\n\nchunks_df = pd.DataFrame(records)\nchunks_df.head(), len(chunks_df)","metadata":{"ExecuteTime":{"end_time":"2026-01-05T05:21:16.796007Z","start_time":"2026-01-05T05:21:16.176407Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:02:54.753648Z","iopub.execute_input":"2026-01-05T15:02:54.754163Z","iopub.status.idle":"2026-01-05T15:02:55.388704Z","shell.execute_reply.started":"2026-01-05T15:02:54.754136Z","shell.execute_reply":"2026-01-05T15:02:55.388054Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(   doc_id  chunk_id                                               text\n 0       0         0  Anovo Anovo (formerly A Novo) is a computer se...\n 1       1         0  Battery indicator A battery indicator (also kn...\n 2       1         1  ad battery when in reality it indicates a prob...\n 3       1         2  s that an internal standby battery needs repla...\n 4       1         3  increase; in many cases the EMF remains more o...,\n 79104)"},"metadata":{}}],"execution_count":4},{"id":"69a8183ec2c4767e","cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nMODEL_NAME = \"intfloat/e5-base-v2\"   # recomendado para retrieval\nmodel = SentenceTransformer(MODEL_NAME)\n\n# Textos a indexar (pasajes)\npassages = [\"passage: \" + t for t in chunks_df[\"text\"].tolist()]\n\n# Embeddings (N x D)\n# normalize_embeddings=True es útil si usarás cosine similarity\nembeddings = model.encode(\n    passages,\n    batch_size=64,\n    show_progress_bar=True,\n    convert_to_numpy=True,\n    normalize_embeddings=True\n).astype(\"float32\")\n\nembeddings.shape, embeddings.dtype","metadata":{"jupyter":{"is_executing":true},"ExecuteTime":{"start_time":"2026-01-05T05:22:02.090309Z"},"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:03:01.849081Z","iopub.execute_input":"2026-01-05T15:03:01.849370Z","iopub.status.idle":"2026-01-05T15:21:24.114132Z","shell.execute_reply.started":"2026-01-05T15:03:01.849344Z","shell.execute_reply":"2026-01-05T15:21:24.113300Z"}},"outputs":[{"name":"stderr","text":"2026-01-05 15:03:16.043407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767625396.272103      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767625396.338171      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767625396.891377      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767625396.891418      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767625396.891421      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767625396.891423      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acd6e8359f114b9a90fa7c8ef222bf31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78b5697614204eb09a3c325ee5df6ea4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cb8497e5e664f43ba81f8f905936121"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d26de30c83e4dae84db8f7615572e1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ed2c0884ac6441cbede2b31b09a2cb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ef8e4d2b58846b095e09dd0a2ebae28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cab063dc41348b3bc4c5fd9d7784d6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c0b17632b234580829b27b5c473aba6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab28aacf72924b68adce9212705dfc0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e49a5ccfe384526b372d559b257be46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1236 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a54d4bd6ec574b47a2c03d49aa539e12"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"((79104, 768), dtype('float32'))"},"metadata":{}}],"execution_count":6},{"id":"1975f9c2556bb8e","cell_type":"code","source":"def embed_query(query: str) -> np.ndarray:\n    q = \"query: \" + query\n    vec = model.encode(\n        [q],\n        convert_to_numpy=True,\n        normalize_embeddings=True\n    ).astype(\"float32\")\n    return vec\n\nquery_text = \"Battery measuring\"\n\nquery_vec = embed_query(query_text)\nquery_vec.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:21:31.134695Z","iopub.execute_input":"2026-01-05T15:21:31.135368Z","iopub.status.idle":"2026-01-05T15:21:31.236902Z","shell.execute_reply.started":"2026-01-05T15:21:31.135338Z","shell.execute_reply":"2026-01-05T15:21:31.236350Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(1, 768)"},"metadata":{}}],"execution_count":7},{"id":"d8fdbebcf1cb69b3","cell_type":"markdown","source":"## Parte 2: FAISS\n\nFAISS es una librería para búsqueda por similitud eficiente y clustering de vectores densos.\n\nLa documentación de FAISS está disponible en este [link](https://faiss.ai/index.html)\n\n### Actividad\n\n1. Crea un índice en FAISS\n2. Carga los embeddings\n3. Realiza una búsqueda a partir de una _query_","metadata":{}},{"id":"5097e7479312e742","cell_type":"code","source":"# código base para FAISS\nimport faiss\nimport numpy as np\n\n# Asumiendo `embeddings` en un array NxD\nindex = faiss.IndexFlatL2(embeddings.shape[1])\nindex.add(embeddings)\n\nD, I = index.search(query_embedding, k=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:21:34.774288Z","iopub.execute_input":"2026-01-05T15:21:34.774687Z","iopub.status.idle":"2026-01-05T15:21:34.781356Z","shell.execute_reply.started":"2026-01-05T15:21:34.774658Z","shell.execute_reply":"2026-01-05T15:21:34.780565Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/251176736.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# código base para FAISS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Asumiendo `embeddings` en un array NxD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"],"ename":"ModuleNotFoundError","evalue":"No module named 'faiss'","output_type":"error"}],"execution_count":8},{"id":"52f15b24eea20878","cell_type":"markdown","source":"## Parte 3 — Vector DB #1: Qdrant (búsqueda vectorial + metadata)\n\n### Objetivo\nRecrear el mismo flujo que con FAISS, pero usando una base vectorial con soporte nativo de **metadata** y filtros.\n\n### Qué debes implementar\n1. Levantar / conectar con una instancia de Qdrant.\n2. Crear una colección con:\n   - dimensión `D` (la de tus embeddings)\n   - métrica (cosine o L2)\n3. Insertar:\n   - `id`\n   - `embedding`\n   - `payload` (metadata: texto, título, etiquetas, etc.)\n4. Consultar Top-k por similitud:\n   - `query_embedding`\n   - `k`\n\n### Inputs esperados (ya definidos arriba en el notebook)\n- `embeddings`: matriz `N x D` (float32)\n- `texts`: lista de `N` strings\n- `metadatas`: lista de `N` dicts (opcional)\n- `query_text`: string\n- `query_embedding`: vector `1 x D`\n\n### Entregable\n- Una función `qdrant_search(query_embedding, k)` que retorne:\n  - lista de `(id, score, text, metadata)`\n- Un ejemplo de consulta con `k=5` y su salida.\n\n### Preguntas\n- ¿La métrica usada fue cosine o L2? ¿Por qué?\n- ¿Qué tan fácil fue filtrar por metadata en comparación con FAISS?\n- ¿Qué pasa con el tiempo de respuesta cuando aumentas `k`?\n","metadata":{}},{"id":"9ff2a78ffffb7836","cell_type":"code","source":"!pip install qdrant-client pymilvus weaviate-client chromadb psycopg2-binary pgvector","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:21:41.124363Z","iopub.execute_input":"2026-01-05T15:21:41.125039Z","iopub.status.idle":"2026-01-05T15:22:01.377950Z","shell.execute_reply.started":"2026-01-05T15:21:41.125007Z","shell.execute_reply":"2026-01-05T15:22:01.377211Z"},"_kg_hide-output":true},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting qdrant-client\n  Downloading qdrant_client-1.16.2-py3-none-any.whl.metadata (11 kB)\nCollecting pymilvus\n  Downloading pymilvus-2.6.6-py3-none-any.whl.metadata (6.8 kB)\nCollecting weaviate-client\n  Downloading weaviate_client-4.19.2-py3-none-any.whl.metadata (3.7 kB)\nCollecting chromadb\n  Downloading chromadb-1.4.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\nCollecting psycopg2-binary\n  Downloading psycopg2_binary-2.9.11-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\nCollecting pgvector\n  Downloading pgvector-0.4.2-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (1.75.1)\nRequirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\nRequirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.0.2)\nCollecting portalocker<4.0,>=2.7.0 (from qdrant-client)\n  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (5.29.5)\nRequirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.12.5)\nRequirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.6.2)\nRequirement already satisfied: setuptools>69 in /usr/local/lib/python3.12/dist-packages (from pymilvus) (75.2.0)\nRequirement already satisfied: orjson>=3.10.15 in /usr/local/lib/python3.12/dist-packages (from pymilvus) (3.11.3)\nRequirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from pymilvus) (1.1.1)\nRequirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.12/dist-packages (from pymilvus) (2.2.2)\nCollecting validators<1.0.0,>=0.34.0 (from weaviate-client)\n  Downloading validators-0.35.0-py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: authlib<2.0.0,>=1.6.5 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (1.6.5)\nCollecting deprecation<3.0.0,>=2.1.0 (from weaviate-client)\n  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\nCollecting pybase64>=1.4.1 (from chromadb)\n  Downloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\nRequirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\nCollecting posthog<6.0.0,>=2.4.0 (from chromadb)\n  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\nCollecting onnxruntime>=1.14.1 (from chromadb)\n  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\nCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n  Downloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\nCollecting pypika>=0.48.9 (from chromadb)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\nRequirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\nCollecting bcrypt>=4.0.1 (from chromadb)\n  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\nRequirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (33.1.0)\nRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\nRequirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\nCollecting mmh3>=4.0.1 (from chromadb)\n  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (14.2.0)\nRequirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\nRequirement already satisfied: cryptography in /usr/local/lib/python3.12/dist-packages (from authlib<2.0.0,>=1.6.5->weaviate-client) (46.0.3)\nRequirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\nRequirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.12.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.11.12)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\nRequirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.3.0)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\nRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.5)\nRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\nRequirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\nRequirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\nRequirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\nRequirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\nRequirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.71.0)\nCollecting opentelemetry-exporter-otlp-proto-common==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n  Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl.metadata (1.8 kB)\nCollecting opentelemetry-proto==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-sdk>=1.2.0 (from chromadb)\n  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb)\n  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk>=1.2.0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.4->pymilvus) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.4->pymilvus) (2025.3)\nCollecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\nRequirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.1)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\nCollecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\nCollecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\nCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\nRequirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\nRequirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.10.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.1rc0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.4)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: cffi>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from cryptography->authlib<2.0.0,>=1.6.5->weaviate-client) (2.0.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=2.0.0->cryptography->authlib<2.0.0,>=1.6.5->weaviate-client) (2.23)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\nDownloading qdrant_client-1.16.2-py3-none-any.whl (377 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.2/377.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pymilvus-2.6.6-py3-none-any.whl (285 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.1/285.1 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading weaviate_client-4.19.2-py3-none-any.whl (603 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m603.7/603.7 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chromadb-1.4.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading psycopg2_binary-2.9.11-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pgvector-0.4.2-py3-none-any.whl (27 kB)\nDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\nDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl (19 kB)\nDownloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl (18 kB)\nDownloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\nDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading validators-0.35.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=453c3a4d13fe42337d9b79e5c17795aa2958779926881fa4c92858b58108ec7b\n  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\nSuccessfully built pypika\nInstalling collected packages: pypika, validators, uvloop, pybase64, psycopg2-binary, portalocker, pgvector, opentelemetry-proto, mmh3, humanfriendly, httptools, deprecation, bcrypt, backoff, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, pymilvus, opentelemetry-semantic-conventions, onnxruntime, weaviate-client, qdrant-client, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb\n  Attempting uninstall: opentelemetry-proto\n    Found existing installation: opentelemetry-proto 1.37.0\n    Uninstalling opentelemetry-proto-1.37.0:\n      Successfully uninstalled opentelemetry-proto-1.37.0\n  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n  Attempting uninstall: opentelemetry-api\n    Found existing installation: opentelemetry-api 1.37.0\n    Uninstalling opentelemetry-api-1.37.0:\n      Successfully uninstalled opentelemetry-api-1.37.0\n  Attempting uninstall: opentelemetry-semantic-conventions\n    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n  Attempting uninstall: opentelemetry-sdk\n    Found existing installation: opentelemetry-sdk 1.37.0\n    Uninstalling opentelemetry-sdk-1.37.0:\n      Successfully uninstalled opentelemetry-sdk-1.37.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 chromadb-1.4.0 coloredlogs-15.0.1 deprecation-2.1.0 httptools-0.7.1 humanfriendly-10.0 mmh3-5.2.0 onnxruntime-1.23.2 opentelemetry-api-1.39.1 opentelemetry-exporter-otlp-proto-common-1.39.1 opentelemetry-exporter-otlp-proto-grpc-1.39.1 opentelemetry-proto-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 pgvector-0.4.2 portalocker-3.2.0 posthog-5.4.0 psycopg2-binary-2.9.11 pybase64-1.4.3 pymilvus-2.6.6 pypika-0.48.9 qdrant-client-1.16.2 uvloop-0.22.1 validators-0.35.0 watchfiles-1.1.1 weaviate-client-4.19.2\n","output_type":"stream"}],"execution_count":9},{"id":"45005299-6810-492a-aa16-16c6cfd93842","cell_type":"code","source":"from qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams, PointStruct\n\n# 1. Inicializar cliente en memoria\nclient_qdrant = QdrantClient(\":memory:\")\n\n# 2. Crear colección (Método actualizado para evitar DeprecationWarning)\ncollection_name = \"wikipedia_chunks\"\nvector_size = embeddings.shape[1]\n\n# Verificamos si existe y la borramos para empezar de cero (equivalente a recreate)\nif client_qdrant.collection_exists(collection_name):\n    client_qdrant.delete_collection(collection_name)\n\nclient_qdrant.create_collection(\n    collection_name=collection_name,\n    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n)\n\n# 3. Preparar los puntos para insertar\npoints = []\nfor i, row in chunks_df.iterrows():\n    points.append(PointStruct(\n        id=i,  # Usamos el índice como ID\n        vector=embeddings[i].tolist(),\n        payload={\"text\": row[\"text\"], \"doc_id\": row[\"doc_id\"], \"chunk_id\": row[\"chunk_id\"]}\n    ))\n\n# Insertar (upsert sigue funcionando igual)\n# La advertencia de \"Local mode\" ignórala, es solo un aviso de rendimiento para datasets grandes\nclient_qdrant.upsert(\n    collection_name=collection_name,\n    points=points\n)\nprint(\"Datos cargados correctamente en Qdrant.\")\n\n# 4. Función de búsqueda CORREGIDA\ndef qdrant_search(query_vec, k=5):\n    # En versiones nuevas, a veces .search() da problemas. \n    # Usamos .query_points() que es el método directo a la API de puntos.\n    search_result = client_qdrant.query_points(\n        collection_name=collection_name,\n        query=query_vec.flatten().tolist(), # Nota: el argumento es 'query', no 'query_vector' aquí\n        limit=k\n    ).points # query_points devuelve un objeto QueryResponse, accedemos a .points\n    \n    results = []\n    for hit in search_result:\n        results.append({\n            \"id\": hit.id,\n            \"score\": hit.score,\n            \"text\": hit.payload[\"text\"],\n            \"metadata\": {\"doc_id\": hit.payload[\"doc_id\"]}\n        })\n    return results\n\n# Prueba\nprint(\"--- Resultados Qdrant (Corregido) ---\")\nresults_qdrant = qdrant_search(query_vec, k=5)\nfor res in results_qdrant:\n    print(f\"ID: {res['id']}, Score: {res['score']:.4f}\\nTexto: {res['text'][:100]}...\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:24:53.196340Z","iopub.execute_input":"2026-01-05T15:24:53.196959Z","iopub.status.idle":"2026-01-05T15:25:47.038592Z","shell.execute_reply.started":"2026-01-05T15:24:53.196929Z","shell.execute_reply":"2026-01-05T15:25:47.037888Z"}},"outputs":[{"name":"stdout","text":"Datos cargados correctamente en Qdrant.\n--- Resultados Qdrant (Corregido) ---\nID: 10176, Score: 0.8703\nTexto: Battery tester A battery tester is an electronic device intended for testing the state of an electri...\n\nID: 1, Score: 0.8618\nTexto: Battery indicator A battery indicator (also known as a battery gauge) is a device which gives inform...\n\nID: 10177, Score: 0.8401\nTexto: ing procedure, according to the type of battery being tested, such as the â€œ421â€ test for lead-ac...\n\nID: 37406, Score: 0.8391\nTexto: ils. One was connected via a series resistor to the battery supply. The second was connected to the ...\n\nID: 71872, Score: 0.8386\nTexto: is achieved. Accepted average float voltages for lead-acid batteries at 25 Â°C can be found in follo...\n\n","output_type":"stream"}],"execution_count":11},{"id":"69bec6e05b842dff","cell_type":"markdown","source":"## Parte 4 — Vector DB #2: Milvus (indexación ANN y escalabilidad)\n\n### Objetivo\nImplementar el flujo de indexación + búsqueda con una base vectorial orientada a escalabilidad.\n\n### Qué debes implementar\n1. Conectar a Milvus.\n2. Crear un esquema (colección) con:\n   - campo `id` (entero o string)\n   - campo `embedding` (vector `D`)\n   - campos de metadata (p.ej., `category`, `source`, `title`)\n3. Insertar `N` embeddings.\n4. Crear/seleccionar un índice ANN (ej. HNSW o IVF).\n5. Ejecutar consultas Top-k y recuperar textos asociados.\n\n### Recomendación didáctica\nHaz dos configuraciones:\n- **Búsqueda exacta** (si aplica) o configuración “más precisa”\n- **Búsqueda ANN** (configuración “más rápida”)\n\nLuego compara:\n- tiempo de consulta\n- overlap de resultados (cuántos IDs coinciden)\n\n### Entregable\n- Función `milvus_search(query_embedding, k)` que devuelva resultados.\n- Un mini experimento: `k=5` y `k=20` (tiempos y resultados).\n\n### Preguntas\n- ¿Qué parámetros del índice/control de búsqueda ajustaste para precisión vs velocidad?\n- ¿Qué evidencia tienes de que ANN cambia los resultados (aunque sea poco)?\n","metadata":{}},{"id":"9109514d-9c80-4879-a221-b474d7249a94","cell_type":"code","source":"# Instalar la versión ligera de Milvus para uso local\n!pip install \"pymilvus[milvus-lite]\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:27:56.897362Z","iopub.execute_input":"2026-01-05T15:27:56.898110Z","iopub.status.idle":"2026-01-05T15:28:02.133218Z","shell.execute_reply.started":"2026-01-05T15:27:56.898077Z","shell.execute_reply":"2026-01-05T15:28:02.132514Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: pymilvus[milvus-lite] in /usr/local/lib/python3.12/dist-packages (2.6.6)\nRequirement already satisfied: setuptools>69 in /usr/local/lib/python3.12/dist-packages (from pymilvus[milvus-lite]) (75.2.0)\nRequirement already satisfied: grpcio!=1.68.0,!=1.68.1,!=1.69.0,!=1.70.0,!=1.70.1,!=1.71.0,!=1.72.1,!=1.73.0,>=1.66.2 in /usr/local/lib/python3.12/dist-packages (from pymilvus[milvus-lite]) (1.75.1)\nRequirement already satisfied: orjson>=3.10.15 in /usr/local/lib/python3.12/dist-packages (from pymilvus[milvus-lite]) (3.11.3)\nRequirement already satisfied: protobuf>=5.27.2 in /usr/local/lib/python3.12/dist-packages (from pymilvus[milvus-lite]) (5.29.5)\nRequirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from pymilvus[milvus-lite]) (1.1.1)\nRequirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.12/dist-packages (from pymilvus[milvus-lite]) (2.2.2)\nCollecting milvus-lite>=2.4.0 (from pymilvus[milvus-lite])\n  Downloading milvus_lite-2.5.1-py3-none-manylinux2014_x86_64.whl.metadata (10.0 kB)\nRequirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio!=1.68.0,!=1.68.1,!=1.69.0,!=1.70.0,!=1.70.1,!=1.71.0,!=1.72.1,!=1.73.0,>=1.66.2->pymilvus[milvus-lite]) (4.15.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from milvus-lite>=2.4.0->pymilvus[milvus-lite]) (4.67.1)\nRequirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.4->pymilvus[milvus-lite]) (2.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.4->pymilvus[milvus-lite]) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.4->pymilvus[milvus-lite]) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.4->pymilvus[milvus-lite]) (2025.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus[milvus-lite]) (1.17.0)\nDownloading milvus_lite-2.5.1-py3-none-manylinux2014_x86_64.whl (55.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: milvus-lite\nSuccessfully installed milvus-lite-2.5.1\n","output_type":"stream"}],"execution_count":13},{"id":"2c0ce6b984802eb6","cell_type":"code","source":"# --- PARTE 4: MILVUS (CORREGIDO CON BATCHING) ---\nfrom pymilvus import MilvusClient\nfrom tqdm.auto import tqdm # Barra de progreso opcional pero útil\n\n# 1. Conectar a Milvus Lite\nclient_milvus = MilvusClient(\"milvus_demo.db\")\n\n# 2. Crear colección\ncollection_name = \"wiki_milvus\"\n\nif client_milvus.has_collection(collection_name):\n    client_milvus.drop_collection(collection_name)\n\nclient_milvus.create_collection(\n    collection_name=collection_name,\n    dimension=embeddings.shape[1],\n    metric_type=\"COSINE\", \n    auto_id=False \n)\n\n# 3. Preparar datos\nprint(\"Preparando datos para Milvus...\")\ndata_milvus = []\nfor i, row in chunks_df.iterrows():\n    data_milvus.append({\n        \"id\": i,\n        \"vector\": embeddings[i].tolist(),\n        \"text\": row[\"text\"],\n        \"doc_id\": row[\"doc_id\"]\n    })\n\n# 4. Insertar datos POR LOTES (Aquí estaba el error)\nbatch_size = 5000 # Insertamos de 5000 en 5000 para no saturar la memoria\nprint(f\"Insertando {len(data_milvus)} registros en lotes de {batch_size}...\")\n\nfor i in tqdm(range(0, len(data_milvus), batch_size)):\n    batch = data_milvus[i : i + batch_size]\n    client_milvus.insert(collection_name=collection_name, data=batch)\n\nprint(\"Inserción completada exitosamente.\")\n\n# 5. Función de búsqueda\ndef milvus_search(query_vec, k=5):\n    search_res = client_milvus.search(\n        collection_name=collection_name,\n        data=[query_vec.flatten().tolist()],\n        limit=k,\n        search_params={\"metric_type\": \"COSINE\", \"params\": {}}, \n        output_fields=[\"text\", \"doc_id\"]\n    )\n    \n    formatted_results = []\n    for hit in search_res[0]:\n        formatted_results.append({\n            \"id\": hit[\"id\"],\n            \"score\": hit[\"distance\"],\n            \"text\": hit[\"entity\"][\"text\"],\n            \"metadata\": {\"doc_id\": hit[\"entity\"][\"doc_id\"]}\n        })\n    return formatted_results\n\n# Prueba\nprint(\"\\n--- Resultados Milvus ---\")\nresults_milvus = milvus_search(query_vec, k=5)\nfor res in results_milvus:\n    print(f\"ID: {res['id']} | Score: {res['score']:.4f}\\nTexto: {res['text'][:100]}...\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:51:22.057165Z","iopub.execute_input":"2026-01-05T15:51:22.057506Z","iopub.status.idle":"2026-01-05T16:07:38.543217Z","shell.execute_reply.started":"2026-01-05T15:51:22.057443Z","shell.execute_reply":"2026-01-05T16:07:38.542456Z"}},"outputs":[{"name":"stdout","text":"Preparando datos para Milvus...\nInsertando 79104 registros en lotes de 5000...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19c6947486414552b46830b5cbac75d8"}},"metadata":{}},{"name":"stdout","text":"Inserción completada exitosamente.\n\n--- Resultados Milvus ---\nID: 10176 | Score: 0.8703\nTexto: Battery tester A battery tester is an electronic device intended for testing the state of an electri...\n\nID: 1 | Score: 0.8618\nTexto: Battery indicator A battery indicator (also known as a battery gauge) is a device which gives inform...\n\nID: 10177 | Score: 0.8401\nTexto: ing procedure, according to the type of battery being tested, such as the â€œ421â€ test for lead-ac...\n\nID: 37406 | Score: 0.8391\nTexto: ils. One was connected via a series resistor to the battery supply. The second was connected to the ...\n\nID: 71872 | Score: 0.8386\nTexto: is achieved. Accepted average float voltages for lead-acid batteries at 25 Â°C can be found in follo...\n\n","output_type":"stream"}],"execution_count":15},{"id":"749c7459cd31499f","cell_type":"markdown","source":"## Parte 5 — Vector DB #3: Weaviate (búsqueda semántica con esquema)\n\n### Objetivo\nMontar una colección con esquema (clase) y ejecutar búsquedas semánticas Top-k, opcionalmente con filtros.\n\n### Qué debes implementar\n1. Conectar a Weaviate.\n2. Definir un esquema:\n   - Clase/colección (por ejemplo `Document`)\n   - Propiedades: `text`, `title`, `category`, etc.\n   - Vector asociado (embedding)\n3. Insertar objetos con:\n   - propiedades + vector\n4. Consultar por similitud (Top-k) con `query_embedding`.\n5. (Opcional) agregar un filtro por propiedad (metadata).\n\n### Recomendación\nAsegúrate de guardar el `text` original y al menos 1 campo de metadata para probar filtrado.\n\n### Entregable\n- Función `weaviate_search(query_embedding, k)` que retorne:\n  - id, score, text, metadata\n\n### Preguntas\n- ¿Qué diferencia conceptual encuentras entre “schema + objetos” vs “tabla + filas”?\n- ¿Cómo describirías el trade-off de complejidad vs expresividad?\n","metadata":{}},{"id":"25195a86a0105c1d","cell_type":"code","source":"import os\nimport logging\nimport warnings\n\n# --- CONFIGURACIÓN PARA SILENCIAR LOGS (EJECUTAR PRIMERO) ---\n# Silenciar warnings de Python (DeprecationWarning, etc.)\nwarnings.filterwarnings(\"ignore\")\n\n# Configurar variables de entorno para que el servidor Weaviate sea menos ruidoso\nos.environ[\"LOG_LEVEL\"] = \"error\"\nos.environ[\"WEAVIATE_LOG_LEVEL\"] = \"error\"\n\n# Silenciar los loggers de las librerías de Python\nlogging.getLogger(\"weaviate\").setLevel(logging.ERROR)\nlogging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n\n# --- CÓDIGO PRINCIPAL WEAVIATE ---\nimport weaviate\nfrom weaviate.classes.config import Configure, Property, DataType\n\nprint(\"Iniciando Weaviate (esto puede tardar unos segundos)...\")\n\n# 1. Conectar a instancia embebida\nclient_wv = weaviate.connect_to_embedded()\n\n# 2. Definir esquema (Collection)\ncollection_name = \"WikiChunk\"\n\n# Limpiar si existe (para reiniciar el ejercicio limpio)\nif client_wv.collections.exists(collection_name):\n    client_wv.collections.delete(collection_name)\n\n# Crear colección con configuración actualizada\nchunks_col = client_wv.collections.create(\n    name=collection_name,\n    vectorizer_config=Configure.Vectorizer.none(), # No usamos vectorizador interno, traemos el nuestro (E5)\n    properties=[\n        Property(name=\"text\", data_type=DataType.TEXT),\n        Property(name=\"doc_id\", data_type=DataType.INT),\n    ]\n)\n\n# 3. Insertar datos\nprint(f\"Insertando {len(chunks_df)} documentos...\")\n# Usamos batch para velocidad\nwith chunks_col.batch.dynamic() as batch:\n    for i, row in chunks_df.iterrows():\n        batch.add_object(\n            properties={\n                \"text\": row[\"text\"],\n                # Aseguramos que sea int nativo de Python para evitar errores de serialización\n                \"doc_id\": int(row[\"doc_id\"]) \n            },\n            vector=embeddings[i].tolist()\n        )\n\n# 4. Función de búsqueda\ndef weaviate_search(query_vec, k=5):\n    chunks = client_wv.collections.get(collection_name)\n    response = chunks.query.near_vector(\n        near_vector=query_vec.flatten().tolist(),\n        limit=k,\n        return_metadata=[\"distance\"]\n    )\n    \n    results = []\n    for obj in response.objects:\n        results.append({\n            \"id\": obj.uuid,\n            # Convertimos distancia a score de similitud (1 - distancia)\n            \"score\": 1 - obj.metadata.distance, \n            \"text\": obj.properties[\"text\"],\n            \"metadata\": {\"doc_id\": obj.properties[\"doc_id\"]}\n        })\n    return results\n\n# Prueba\nprint(\"\\n--- Resultados Weaviate ---\")\nresults_wv = weaviate_search(query_vec, k=5)\nfor res in results_wv:\n    print(f\"Score (Sim): {res['score']:.4f} | Texto: {res['text'][:100]}...\")\n\n# Cerrar cliente al terminar (IMPORTANTE)\nclient_wv.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T16:38:00.410567Z","iopub.execute_input":"2026-01-05T16:38:00.411728Z","iopub.status.idle":"2026-01-05T16:39:23.614158Z","shell.execute_reply.started":"2026-01-05T16:38:00.411697Z","shell.execute_reply":"2026-01-05T16:39:23.613329Z"}},"outputs":[{"name":"stderr","text":"INFO:weaviate-client:Started /root/.cache/weaviate-embedded: process ID 243\n","output_type":"stream"},{"name":"stdout","text":"Iniciando Weaviate (esto puede tardar unos segundos)...\n","output_type":"stream"},{"name":"stderr","text":"{\"action\":\"load_all_shards\",\"build_git_commit\":\"\",\"build_go_version\":\"go1.24.3\",\"build_image_tag\":\"\",\"build_wv_version\":\"1.30.5\",\"level\":\"error\",\"msg\":\"failed to load all shards: context canceled\",\"time\":\"2026-01-05T16:38:02Z\"}\n","output_type":"stream"},{"name":"stdout","text":"Insertando 79104 documentos...\n\n--- Resultados Weaviate ---\nScore (Sim): 0.8703 | Texto: Battery tester A battery tester is an electronic device intended for testing the state of an electri...\nScore (Sim): 0.8618 | Texto: Battery indicator A battery indicator (also known as a battery gauge) is a device which gives inform...\nScore (Sim): 0.8401 | Texto: ing procedure, according to the type of battery being tested, such as the â€œ421â€ test for lead-ac...\nScore (Sim): 0.8391 | Texto: ils. One was connected via a series resistor to the battery supply. The second was connected to the ...\nScore (Sim): 0.8386 | Texto: is achieved. Accepted average float voltages for lead-acid batteries at 25 Â°C can be found in follo...\n","output_type":"stream"},{"name":"stderr","text":"{\"build_git_commit\":\"\",\"build_go_version\":\"go1.24.3\",\"build_image_tag\":\"\",\"build_wv_version\":\"1.30.5\",\"error\":\"cannot find peer\",\"level\":\"error\",\"msg\":\"transferring leadership\",\"time\":\"2026-01-05T16:39:21Z\"}\n","output_type":"stream"}],"execution_count":17},{"id":"40919fda773f0fbb","cell_type":"markdown","source":"## Parte 6 — Vector Store #4: Chroma (prototipado rápido)\n\n### Objetivo\nImplementar la misma idea de indexación y búsqueda semántica con una herramienta ligera de prototipado.\n\n### Qué debes implementar\n1. Crear una colección.\n2. Insertar:\n   - ids\n   - embeddings\n   - documents (texto)\n   - metadatas (opcional)\n3. Consultar Top-k con `query_embedding`.\n\n### Nota didáctica\nChroma es útil para prototipos: enfócate en reproducir el pipeline sin “infra pesada”.\n\n### Entregable\n- Función `chroma_search(query_embedding, k)` que retorne resultados.\n- Una consulta con `k=5`.\n\n### Preguntas\n- ¿Qué tan fácil fue implementar todo comparado con Qdrant/Milvus?\n- ¿Qué limitaciones ves para un sistema en producción?\n","metadata":{}},{"id":"52eb94e7d911247b","cell_type":"code","source":"# --- PARTE 6: CHROMA DB ---\nimport chromadb\nfrom tqdm.auto import tqdm\n\nprint(\"Iniciando ChromaDB...\")\n\n# 1. Cliente en memoria (se borra al reiniciar)\nclient_chroma = chromadb.Client()\n\n# 2. Crear colección\ncollection_name = \"wiki_chroma\"\n# Borramos si existe para empezar limpio\ntry:\n    client_chroma.delete_collection(collection_name)\nexcept:\n    pass \n\n# \"hnsw:space\": \"cosine\" es importante para usar similitud Coseno\ncollection = client_chroma.create_collection(\n    name=collection_name, \n    metadata={\"hnsw:space\": \"cosine\"} \n)\n\n# 3. Preparar datos\n# Chroma necesita listas separadas\nids = [str(i) for i in chunks_df.index] # Los IDs deben ser strings\nembeds_list = embeddings.tolist()\ndocs_list = chunks_df[\"text\"].tolist()\nmetas_list = [{\"doc_id\": int(r[\"doc_id\"]), \"chunk_id\": int(r[\"chunk_id\"])} for i, r in chunks_df.iterrows()]\n\n# 4. Insertar en lotes (Batching de 5000)\nbatch_size = 5000\nprint(f\"Insertando {len(ids)} documentos...\")\n\nfor i in tqdm(range(0, len(ids), batch_size)):\n    end = i + batch_size\n    collection.add(\n        ids=ids[i:end],\n        embeddings=embeds_list[i:end],\n        documents=docs_list[i:end],\n        metadatas=metas_list[i:end]\n    )\n\n# 5. Función de búsqueda\ndef chroma_search(query_vec, k=5):\n    results = collection.query(\n        query_embeddings=[query_vec.flatten().tolist()],\n        n_results=k\n    )\n    \n    formatted = []\n    # Chroma devuelve listas de listas\n    for i in range(len(results[\"ids\"][0])):\n        formatted.append({\n            \"id\": results[\"ids\"][0][i],\n            # NOTA: Chroma devuelve DISTANCIA (menor es mejor), no similitud.\n            \"score\": results[\"distances\"][0][i], \n            \"text\": results[\"documents\"][0][i],\n            \"metadata\": results[\"metadatas\"][0][i]\n        })\n    return formatted\n\n# Prueba\nprint(\"\\n--- Resultados Chroma ---\")\nresults_chroma = chroma_search(query_vec, k=5)\nfor res in results_chroma:\n    # Una distancia baja (ej: 0.12) equivale a una similitud alta (0.88)\n    print(f\"Distancia: {res['score']:.4f} | Texto: {res['text'][:100]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T16:44:37.036040Z","iopub.execute_input":"2026-01-05T16:44:37.036327Z","iopub.status.idle":"2026-01-05T16:46:22.859699Z","shell.execute_reply.started":"2026-01-05T16:44:37.036302Z","shell.execute_reply":"2026-01-05T16:46:22.858936Z"}},"outputs":[{"name":"stdout","text":"Iniciando ChromaDB...\nInsertando 79104 documentos...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43bee599a9954ca4821e42b45b48f4d6"}},"metadata":{}},{"name":"stdout","text":"\n--- Resultados Chroma ---\nDistancia: 0.1297 | Texto: Battery tester A battery tester is an electronic device intended for testing the state of an electri...\nDistancia: 0.1382 | Texto: Battery indicator A battery indicator (also known as a battery gauge) is a device which gives inform...\nDistancia: 0.1599 | Texto: ing procedure, according to the type of battery being tested, such as the â€œ421â€ test for lead-ac...\nDistancia: 0.1609 | Texto: ils. One was connected via a series resistor to the battery supply. The second was connected to the ...\nDistancia: 0.1614 | Texto: is achieved. Accepted average float voltages for lead-acid batteries at 25 Â°C can be found in follo...\n","output_type":"stream"}],"execution_count":18},{"id":"f7b383f0d5f720d","cell_type":"markdown","source":"## Parte 7 — SQL + vectores: PostgreSQL/pgvector (vector search transparente)\n\n### Objetivo\nGuardar embeddings en una tabla y ejecutar una consulta SQL de similitud.\n\n### Qué debes implementar\n1. Conectar a una base PostgreSQL con `pgvector` habilitado.\n2. Crear una tabla (ej. `documents`) con:\n   - `id` (PK)\n   - `text` (texto)\n   - `embedding` (vector(D))\n   - metadata (columnas adicionales)\n3. Insertar todos los documentos y embeddings.\n4. Consultar Top-k por similitud, ordenando por distancia.\n\n### Fórmula conceptual (lo que implementa tu SQL)\nPara una consulta `q`, buscas:\n$$ argmin_d \\in D \\; \\text{dist}(\\vec{q}, \\vec{d})$$\ndonde `dist` puede ser L2 o una variante para cosine (según configuración).\n\n### Entregable\n- Función `pgvector_search(query_embedding, k)` que ejecute SQL y devuelva:\n  - id, score/distancia, text, metadata\n\n### Preguntas\n- ¿Qué tan “explicable” te parece esta aproximación vs las otras?\n- ¿Qué ventajas ofrece el mundo SQL (JOIN, filtros, agregaciones)?\n- ¿Qué limitaciones esperas en escalabilidad frente a bases vectoriales dedicadas?\n","metadata":{}},{"id":"c3b4b4b96bb8b8bb","cell_type":"code","source":"# --- PARTE 7: POSTGRESQL + PGVECTOR ---\nimport psycopg2\nimport numpy as np\n\n# Configuración de conexión (Simulada o Real)\n# Si estás en Kaggle/Colab, esto probablemente fallará y saltará al 'except'\nDB_CONFIG = {\n    \"host\": \"localhost\",\n    \"database\": \"postgres\",\n    \"user\": \"postgres\",\n    \"password\": \"mysecretpassword\", # Contraseña genérica\n    \"port\": \"5432\"\n}\n\nprint(\"Iniciando Parte 7: PostgreSQL + pgvector...\")\n\ntry:\n    # 1. Intentar conexión\n    conn = psycopg2.connect(**DB_CONFIG)\n    cur = conn.cursor()\n    \n    # 2. Setup (Habilitar extensión y tabla)\n    # Esto habilita la matemática vectorial dentro de SQL\n    cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n    cur.execute(\"DROP TABLE IF EXISTS documents\")\n    \n    dim = embeddings.shape[1] # 768 para E5-base\n    cur.execute(f\"CREATE TABLE documents (id bigserial PRIMARY KEY, text text, doc_id int, embedding vector({dim}))\")\n    \n    # 3. Insertar datos (Muestra de 100 para probar rápido)\n    print(\"Conexión exitosa. Insertando muestra de datos...\")\n    data_sql = []\n    for i in range(100): \n        row = chunks_df.iloc[i]\n        # pgvector recibe listas de python y las convierte automáticamente\n        data_sql.append((row[\"text\"], int(row[\"doc_id\"]), embeddings[i].tolist()))\n    \n    cur.executemany(\"INSERT INTO documents (text, doc_id, embedding) VALUES (%s, %s, %s)\", data_sql)\n    conn.commit()\n\n    # 4. Búsqueda SQL\n    # El operador <=> calcula la Distancia Coseno\n    query_sql = f\"\"\"\n        SELECT text, doc_id, embedding <=> %s::vector AS distance\n        FROM documents\n        ORDER BY distance ASC\n        LIMIT 5\n    \"\"\"\n    cur.execute(query_sql, (query_vec.flatten().tolist(),))\n    rows = cur.fetchall()\n    \n    print(\"\\n--- Resultados PGVector (SQL Real) ---\")\n    for r in rows:\n        print(f\"Distancia: {r[2]:.4f} | Texto: {r[0][:100]}...\")\n\n    cur.close()\n    conn.close()\n\nexcept Exception as e:\n    # ESTO ES LO QUE PROBABLEMENTE VERÁS SI NO TIENES DOCKER/POSTGRES INSTALADO\n    print(\"\\n⚠️ AVISO: No se detectó una base de datos PostgreSQL local.\")\n    print(f\"Detalle: {e}\")\n    print(\"\\n\" + \"=\"*50)\n    print(\"RESUMEN TEÓRICO PARA EL INFORME (Parte 7)\")\n    print(\"=\"*50)\n    print(\"Como no hay servidor SQL disponible, aquí tienes la lógica de cómo funciona:\")\n    print(\"\\n1. ALMACENAMIENTO:\")\n    print(\"   Postgres usa un tipo de dato especial 'vector(768)' que guarda el array de floats.\")\n    print(\"   Se guarda junto a tus columnas normales (id, texto, fecha) en la misma tabla.\")\n    print(\"\\n2. BÚSQUEDA (La Query Mágica):\")\n    print(\"   SELECT * FROM documents ORDER BY embedding <=> '[0.1, 0.2, ...]' LIMIT 5;\")\n    print(\"   - El operador '<=>' calcula la distancia Coseno.\")\n    print(\"   - El 'ORDER BY' encuentra los vectores más cercanos.\")\n    print(\"\\n3. VENTAJAS:\")\n    print(\"   - Puedes hacer JOINS: 'Dame documentos similares PERO solo del usuario X'.\")\n    print(\"   - No necesitas mover datos de una DB a otra (todo vive en SQL).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T16:47:25.752209Z","iopub.execute_input":"2026-01-05T16:47:25.752922Z","iopub.status.idle":"2026-01-05T16:47:25.762059Z","shell.execute_reply.started":"2026-01-05T16:47:25.752891Z","shell.execute_reply":"2026-01-05T16:47:25.761495Z"}},"outputs":[{"name":"stdout","text":"Iniciando Parte 7: PostgreSQL + pgvector...\n\n⚠️ AVISO: No se detectó una base de datos PostgreSQL local.\nDetalle: connection to server at \"localhost\" (::1), port 5432 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused\n\tIs the server running on that host and accepting TCP/IP connections?\n\n\n==================================================\nRESUMEN TEÓRICO PARA EL INFORME (Parte 7)\n==================================================\nComo no hay servidor SQL disponible, aquí tienes la lógica de cómo funciona:\n\n1. ALMACENAMIENTO:\n   Postgres usa un tipo de dato especial 'vector(768)' que guarda el array de floats.\n   Se guarda junto a tus columnas normales (id, texto, fecha) en la misma tabla.\n\n2. BÚSQUEDA (La Query Mágica):\n   SELECT * FROM documents ORDER BY embedding <=> '[0.1, 0.2, ...]' LIMIT 5;\n   - El operador '<=>' calcula la distancia Coseno.\n   - El 'ORDER BY' encuentra los vectores más cercanos.\n\n3. VENTAJAS:\n   - Puedes hacer JOINS: 'Dame documentos similares PERO solo del usuario X'.\n   - No necesitas mover datos de una DB a otra (todo vive en SQL).\n","output_type":"stream"}],"execution_count":20}]}