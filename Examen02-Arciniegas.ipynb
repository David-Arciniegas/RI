{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14608380,"sourceType":"datasetVersion","datasetId":612177}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-28T16:43:18.499833Z","iopub.execute_input":"2026-01-28T16:43:18.500167Z","iopub.status.idle":"2026-01-28T16:43:18.510835Z","shell.execute_reply.started":"2026-01-28T16:43:18.500141Z","shell.execute_reply":"2026-01-28T16:43:18.510294Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!pip install sentence-transformers faiss-cpu nltk kagglehub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T16:44:50.398848Z","iopub.execute_input":"2026-01-28T16:44:50.399316Z","iopub.status.idle":"2026-01-28T16:44:53.816233Z","shell.execute_reply.started":"2026-01-28T16:44:50.399280Z","shell.execute_reply":"2026-01-28T16:44:53.815298Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\nRequirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.2)\nRequirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.2)\nRequirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.4.0)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (26.0rc2)\nRequirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\nRequirement already satisfied: kagglesdk<1.0,>=0.1.14 in /usr/local/lib/python3.12/dist-packages (from kagglehub) (0.1.14)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub) (2.32.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.1rc0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kagglesdk<1.0,>=0.1.14->kagglehub) (5.29.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2026.1.4)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!pip install -q ir_measures","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T16:24:12.962655Z","iopub.execute_input":"2026-01-28T16:24:12.963499Z","iopub.status.idle":"2026-01-28T16:24:16.004536Z","shell.execute_reply.started":"2026-01-28T16:24:12.963464Z","shell.execute_reply":"2026-01-28T16:24:16.003588Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Examen Final Arciniegas","metadata":{}},{"cell_type":"markdown","source":"## Parte 1: Importaciones","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install faiss-gpu sentence-transformers\n\nimport pandas as pd\nimport numpy as np\nimport json\nimport os\nimport nltk\nimport faiss\nfrom sentence_transformers import SentenceTransformer, CrossEncoder\nfrom tqdm.notebook import tqdm\nimport torch\n\n# Configuración del dispositivo\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Usando dispositivo: {device}\")\n\n# Descargas silenciosas de NLTK\nnltk.download('stopwords', quiet=True)\nnltk.download('punkt', quiet=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T16:48:36.054242Z","iopub.execute_input":"2026-01-28T16:48:36.054580Z","iopub.status.idle":"2026-01-28T16:48:37.321097Z","shell.execute_reply.started":"2026-01-28T16:48:36.054547Z","shell.execute_reply":"2026-01-28T16:48:37.320365Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Parte 2: carga de corpus, queries y qrels","metadata":{}},{"cell_type":"code","source":"json_file_path = '/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json'\ntry:\n    df = pd.read_json(json_file_path, lines=True, nrows=10000)\n    print(f\"Dataset cargado. Filas: {len(df)}\")\n    \nexcept ValueError as e:\n    print(\"No se encontró el archivo.\")\n    raise e\n\n# Inicializamos estructuras\ndocs_store = {}\nqueries_store = {}\nqrels_store = {}\n\nprint(\"Generando documentos\")\n\n# generar docs_store\nfor index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Procesando Docs\"):\n    doc_id = str(row['id'])\n    title = str(row.get('title', '')).replace('\\n', ' ').strip()\n    abstract = str(row.get('abstract', '')).replace('\\n', ' ').strip()\n    \n    docs_store[doc_id] = f\"{title}. {abstract}\"\n\n# generar queries y qrels\nsample_queries = df.sample(20, random_state=42)\n\nfor index, row in sample_queries.iterrows():\n    qid = f\"Q_{row['id']}\"   \n    doc_id = str(row['id'])  \n    queries_store[qid] = str(row['title']).replace('\\n', ' ').strip()\n    qrels_store[qid] = {doc_id: 1}\n\nprint(f\"Total Documentos (docs_store): {len(docs_store)}\")\nprint(f\"Total Consultas (queries_store): {len(queries_store)}\")\nprint(f\"Total Qrels (qrels_store):       {len(qrels_store)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:22:40.664919Z","iopub.execute_input":"2026-01-28T17:22:40.665330Z","iopub.status.idle":"2026-01-28T17:22:41.287879Z","shell.execute_reply.started":"2026-01-28T17:22:40.665300Z","shell.execute_reply":"2026-01-28T17:22:41.287096Z"}},"outputs":[{"name":"stdout","text":"Dataset cargado. Filas: 10000\nGenerando documentos\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Procesando Docs:   0%|          | 0/10000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee5aa8a47fe64b65a568d725ec8d4fd5"}},"metadata":{}},{"name":"stdout","text":"Total Documentos (docs_store): 10000\nTotal Consultas (queries_store): 20\nTotal Qrels (qrels_store):       20\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"## Parte 3: Preprocesamiento\n\nEliminacion de stopwords, normalizacio y stemming","metadata":{}},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nstemmer = PorterStemmer()\ntranslator = str.maketrans('', '', string.punctuation)\n\ndef preprocess_text(text):\n    # Normalización\n    text = text.lower()\n    text = text.translate(translator)\n    \n    # Tokenizacion\n    tokens = text.split()\n    \n    # Stopwords y Stemming\n    cleaned_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n    \n    return \" \".join(cleaned_tokens)\n\nsample_text = \"International conflict and economic policy news.\"\nprint(f\"Original: {sample_text}\")\nprint(f\"Procesado: {preprocess_text(sample_text)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T16:48:55.806321Z","iopub.execute_input":"2026-01-28T16:48:55.806636Z","iopub.status.idle":"2026-01-28T16:48:55.815105Z","shell.execute_reply.started":"2026-01-28T16:48:55.806609Z","shell.execute_reply":"2026-01-28T16:48:55.814421Z"}},"outputs":[{"name":"stdout","text":"Original: International conflict and economic policy news.\nProcesado: intern conflict econom polici news\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## Parte 4: Embeddings y FAISS","metadata":{}},{"cell_type":"code","source":"model_embedding = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n\ndoc_ids = list(docs_store.keys())\n\n\nprint(\"Preprocesando\")\npassages = [preprocess_text(docs_store[did]) for did in tqdm(doc_ids, desc=\"Preprocesando\")]\n\n# Generar Embeddings\nprint(\"Generando embeddings\")\ndoc_embeddings = model_embedding.encode(\n    passages, \n    batch_size=128, \n    show_progress_bar=True, \n    convert_to_numpy=True, \n    normalize_embeddings=True\n)\n\n# FAISS\nd = doc_embeddings.shape[1]\nindex = faiss.IndexFlatIP(d) \nindex.add(doc_embeddings)\nprint(f\"FAISS creado con {index.ntotal} documentos.\")\n\ndef retrieve_candidates(query_text, top_k=50):\n    query_processed = preprocess_text(query_text) \n    q_embed = model_embedding.encode([query_processed], convert_to_numpy=True, normalize_embeddings=True)\n    D, I = index.search(q_embed, top_k)\n    \n    results = {}\n    for score, idx in zip(D[0], I[0]):\n        if idx < len(doc_ids): \n            results[doc_ids[idx]] = float(score)\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T16:53:30.805336Z","iopub.execute_input":"2026-01-28T16:53:30.805982Z","iopub.status.idle":"2026-01-28T16:53:55.961321Z","shell.execute_reply.started":"2026-01-28T16:53:30.805950Z","shell.execute_reply":"2026-01-28T16:53:55.960688Z"}},"outputs":[{"name":"stdout","text":"Preprocesando\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Preprocesando:   0%|          | 0/10000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e3a576dc55d4a76bc5b16bc21cbc94e"}},"metadata":{}},{"name":"stdout","text":"Generando embeddings\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/79 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31ab84a7016244f497dbd112db1fad32"}},"metadata":{}},{"name":"stdout","text":"FAISS creado con 10000 documentos.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"## Parte 5: Re Rankings - Cross enconder","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ncross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device=device)\n\n\ndef rerank_results(query_text, initial_results_dict, top_k=50):\n    if not initial_results_dict:\n        return {}\n    \n    pairs = []\n    doc_ids_list = []\n    \n    for doc_id in list(initial_results_dict.keys())[:top_k]:\n        doc_text = docs_store.get(doc_id, \"\")\n        pairs.append([query_text, doc_text])\n        doc_ids_list.append(doc_id)\n    \n    if not pairs: return {}\n    scores = cross_encoder.predict(pairs, show_progress_bar=False)\n    \n    reranked = {}\n    for i, doc_id in enumerate(doc_ids_list):\n        reranked[doc_id] = float(scores[i])\n        \n    # Ordenar descendente\n    return dict(sorted(reranked.items(), key=lambda x: x[1], reverse=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:20:07.597788Z","iopub.execute_input":"2026-01-28T17:20:07.598138Z","iopub.status.idle":"2026-01-28T17:20:09.819900Z","shell.execute_reply.started":"2026-01-28T17:20:07.598093Z","shell.execute_reply":"2026-01-28T17:20:09.819155Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"id_demo = list(queries_store.keys())[0]\nquery_demo = queries_store[qid_demo]\n\nprint(f\"Consulta ID: {qid_demo}\")\nprint(f\"Texto: '{query_demo}'\")\n\nres_stage1 = retrieve_candidates(query_demo, top_k=50)\n\nres_stage2 = rerank_results(query_demo, res_stage1, top_k=50)\n\nprint(\"\\nComparativa Top-5 Documentos:\")\nprint(f\"{'Pos':<4} | {'ID (FAISS)':<15} {'Score':<10} || {'ID (Re-rank)':<15} {'Score':<10}\")\nprint(\"-\" * 65)\n\ntop_ids_1 = list(res_stage1.items())[:5]\ntop_ids_2 = list(res_stage2.items())[:5]\n\nfor i in range(5):\n    id1, sc1 = top_ids_1[i]\n    id2, sc2 = top_ids_2[i]\n    print(f\"{i+1:<4} | {id1:<15} {sc1:.4f}     || {id2:<15} {sc2:.4f}\")\n\ncorrect_doc = list(qrels_store[qid_demo].keys())[0]\nrank_pos = list(res_stage2.keys()).index(correct_doc) + 1 if correct_doc in res_stage2 else \">50\"\nprint(f\"\\nEl documento({correct_doc}) quedó en la posición: {rank_pos}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:21:35.125232Z","iopub.execute_input":"2026-01-28T17:21:35.125545Z","iopub.status.idle":"2026-01-28T17:21:35.248392Z","shell.execute_reply.started":"2026-01-28T17:21:35.125517Z","shell.execute_reply":"2026-01-28T17:21:35.247836Z"}},"outputs":[{"name":"stdout","text":"Consulta ID: Q_705.225\nTexto: 'Topological Quiver Matrix Models and Quantum Foam'\n\nComparativa Top-5 Documentos:\nPos  | ID (FAISS)      Score      || ID (Re-rank)    Score     \n-----------------------------------------------------------------\n1    | 705.225         0.8188     || 705.225         9.9021\n2    | 705.3236        0.4935     || 704.0278        0.8805\n3    | 704.1712        0.4735     || 705.3892        0.4875\n4    | 704.0278        0.4619     || 705.1645        -0.7480\n5    | 704.0796        0.4544     || 704.1291        -1.8485\n\nEl documento(705.225) quedó en la posición: 1\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"## Parte 6: Evaluacion del sistema y simulacion de consulta","metadata":{}},{"cell_type":"code","source":"\ndef calculate_metrics(results_dict, qrels_dict, k=10):\n    # Tomamos los top-k \n    top_hits = list(results_dict.keys())[:k]\n    relevant_retrieved = sum([1 for doc in top_hits if qrels_dict.get(doc, 0) > 0])\n    total_relevant = sum([1 for rel in qrels_dict.values() if rel > 0])\n    \n    # Precision\n    precision = relevant_retrieved / k\n    \n    # Recall\n    recall = relevant_retrieved / total_relevant if total_relevant > 0 else 0.0\n    \n    return precision, recall\n\nprint(f\"Evaluando el sistema sobre {len(queries_store)} consultas de prueba...\")\n\nmetrics_data = []\nfor qid, query_text in tqdm(queries_store.items(), desc=\"Evaluando\"):\n    qrels_query = qrels_store.get(qid, {})\n    if not qrels_query: continue\n    \n    # recuperación FAISS  50\n    res_stage1 = retrieve_candidates(query_text, top_k=50)\n    p10_s1, r10_s1 = calculate_metrics(res_stage1, qrels_query, k=10)\n    \n    # Cross-Encoder\n    res_stage2 = rerank_results(query_text, res_stage1, top_k=50)\n    p10_s2, r10_s2 = calculate_metrics(res_stage2, qrels_query, k=10)\n    \n    metrics_data.append({\n        \"Query ID\": qid,\n        \"P@10 (Inicial)\": p10_s1,\n        \"R@10 (Inicial)\": r10_s1,\n        \"P@10 (Re-rank)\": p10_s2,\n        \"R@10 (Re-rank)\": r10_s2\n    })\n\ndf_metrics = pd.DataFrame(metrics_data)\n\n\nprint(f\"Promedio Precision@10 (Inicial): {df_metrics['P@10 (Inicial)'].mean():.4f}\")\nprint(f\"Promedio Precision@10 (Final):   {df_metrics['P@10 (Re-rank)'].mean():.4f}\")\nprint(f\"Promedio Recall@10 (Inicial):    {df_metrics['R@10 (Inicial)'].mean():.4f}\")\nprint(f\"Promedio Recall@10 (Final):      {df_metrics['R@10 (Re-rank)'].mean():.4f}\")\n\nprint(\"\\nDetalle por consulta (Primeras 5):\")\ndisplay(df_metrics.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T17:08:23.555991Z","iopub.execute_input":"2026-01-28T17:08:23.556627Z","iopub.status.idle":"2026-01-28T17:08:26.053623Z","shell.execute_reply.started":"2026-01-28T17:08:23.556599Z","shell.execute_reply":"2026-01-28T17:08:26.052865Z"}},"outputs":[{"name":"stdout","text":"Evaluando el sistema sobre 20 consultas de prueba...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluando:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cca29c14aa484d0991bde77cd85e0bbe"}},"metadata":{}},{"name":"stdout","text":"Promedio Precision@10 (Inicial): 0.0950\nPromedio Precision@10 (Final):   0.1000\nPromedio Recall@10 (Inicial):    0.9500\nPromedio Recall@10 (Final):      1.0000\n\nDetalle por consulta (Primeras 5):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"     Query ID  P@10 (Inicial)  R@10 (Inicial)  P@10 (Re-rank)  R@10 (Re-rank)\n0   Q_705.225             0.1             1.0             0.1             1.0\n1  Q_705.0682             0.0             0.0             0.1             1.0\n2  Q_704.1732             0.1             1.0             0.1             1.0\n3   Q_705.074             0.1             1.0             0.1             1.0\n4  Q_705.0519             0.1             1.0             0.1             1.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Query ID</th>\n      <th>P@10 (Inicial)</th>\n      <th>R@10 (Inicial)</th>\n      <th>P@10 (Re-rank)</th>\n      <th>R@10 (Re-rank)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Q_705.225</td>\n      <td>0.1</td>\n      <td>1.0</td>\n      <td>0.1</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Q_705.0682</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.1</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Q_704.1732</td>\n      <td>0.1</td>\n      <td>1.0</td>\n      <td>0.1</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Q_705.074</td>\n      <td>0.1</td>\n      <td>1.0</td>\n      <td>0.1</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Q_705.0519</td>\n      <td>0.1</td>\n      <td>1.0</td>\n      <td>0.1</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":35},{"cell_type":"markdown","source":"## Parte 7: Analisis\n\nEn los resultados se puede observar una precisión de 0.1000 que parece muy baja, pero en esta simulación representa un puntaje perfecto. Esto se debe a que para cada consulta de prueba definimos que existe exactamente un documento relevante. Ademas, si el sistema devuelve 10 resultados y solo 1 es correcto, la nota máxima posible es 1/10 = 0.10. Por lo tanto, obtener este valor significa que el sistema encontró la respuesta correcta el 100% de las veces y la colocó exitosamente dentro de la primera página de resultados demostrando una busqueda exitos.\n\n\nLa comparación entre etapas demuestra por qué el re-ranking es vital. Mientras que la búsqueda inicial atrapa candidatos rápidamente, a veces deja el documento correcto en posiciones bajas. La etapa de Re-ranking revisó esos candidatos y rescató los documentos relevantes, subiéndolos a las primeras posiciones. Esto se evidencia en que el Recall, ya que se mantuvo en 1.0, asegurando que el usuario final siempre vea la respuesta correcta en el top 10, corrigiendo cualquier error de ordenamiento.","metadata":{}}]}